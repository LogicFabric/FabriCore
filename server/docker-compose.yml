services:
  server:
    build: .
    ports:
      - "8000:8000"
    # Clear previous model config before starting to prevent auto-loading
    command: sh -c "rm -f /server/llm_models/llama_args.txt && uvicorn app.main:app --host 0.0.0.0 --port 8000"
    environment:
      - PROJECT_NAME=FabriCore
      - DATABASE_URL=postgresql://fabricore:securepassword@db:5432/fabricore
      - SECRET_KEY=change_me_in_production
      - MODELS_PATH=/server/llm_models
      - LLAMA_BASE_URL=http://llama:8080
    depends_on:
      db:
        condition: service_healthy
      llama:
        condition: service_started
    volumes:
      - .:/server
      - ./llm_models:/server/llm_models
      - /var/run/docker.sock:/var/run/docker.sock
    # No longer needs GPU passthrough directly if llama handles it
    # But keeping it for potential other server-side GPU needs
    devices:
      - /dev/dri:/dev/dri

  llama:
    build:
      context: .
      dockerfile: Dockerfile.llama
    ports:
      - "8080:8080"
    volumes:
      - ./llm_models:/app/llm_models
    devices:
      - /dev/dri:/dev/dri
    environment:
      - GGML_VULKAN=1
    # Command will be overridden by ModelManager during switching
    command: [ "--model", "/app/llm_models/placeholder.gguf", "--host", "0.0.0.0", "--port", "8080", "--n-gpu-layers", "-1" ]

  db:
    image: postgres:15-alpine
    restart: always
    environment:
      - POSTGRES_USER=fabricore
      - POSTGRES_PASSWORD=securepassword
      - POSTGRES_DB=fabricore
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U fabricore -d fabricore" ]
      interval: 5s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:
